{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Homework 6\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, November 2nd, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW6.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import mode\n",
    "from sklearn import linear_model\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Last, First]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS 109a or STATS 121a or AC 209a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[First and Land names of students with whom you have collaborated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Recommender System for Movies\n",
    "\n",
    "In this problem, you will build a model to recommend movies using ratings from users. \n",
    "\n",
    "The dataset for this problem is contained in `dataset_4_ratings.txt`. This dataset contains ratings from 100 users for 1000 movies. The first two columns contain the user and movie IDs. The last column contains a 1 if the user liked the movie, and 0 otherwise. Not every movie is rated by every user (i.e. some movies have more ratings than others).\n",
    "\n",
    "The names of the movies corresponding to the IDs are provided in `dataset_4_movie_names.txt`.\n",
    "\n",
    "### Part 1(a): Exploring how to rank\n",
    "\n",
    "One way of recommending movies is to recommend movies that are generally agreed upon to be good. But how do we measure the \"goodness\" or \"likability\" of a movie?\n",
    "\n",
    "\n",
    "- **Implementation:** Suppose we measure the \"goodness\" of a movie by the probability that it will be liked by a user, $P(\\textbf{label} = \\text{like}|\\textbf{movie}) = \\theta_{\\text{movie}}$. Assuming that each user independently rates a given movie according to the probability $\\theta_{\\text{movies}}$. Use a reasonable estimate of $\\theta_{\\text{movies}}$ to build a list of top 25 movies that you would recommend to a new user.\n",
    "\n",
    "   **Hint:** What does the likelihood function, $P(\\textbf{likes} = k | \\theta_{\\text{movie}}, n, \\textbf{movie})$, look like? What $\\theta_{\\text{movie}}$ will maximize the likelihood?\n",
    "   \n",
    "\n",
    "- **Analysis:** Why is using $\\theta_{\\text{movie}}$ to rank movies more appropriate than using the total number of likes? Explain why your estimate of $\\theta_{\\text{movie}}$ is reasonable. Explain the potential draw backs of estimating $\\theta_{\\text{movie}}$ this way.\n",
    "\n",
    "   **Hint:** Under what conditions may models that maximize the likelihood be suboptimal? Do those conditions apply here?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1(b): Exploring the effect of prior beliefs\n",
    "\n",
    "Let's add a prior, $p(\\theta_{\\text{movie}})$, to our probabilistic model for movie rating. To keep things simple, we will restrict ourselves to using beta priors.\n",
    "\n",
    "- **Analysis:** How might adding a prior to our model benifit us in our specific task? Why are beta distributions appropriate priors for our application?\n",
    "\n",
    "  **Hint:** Try visualizing beta priors $a = b = 1$, $a = b = 0.5$, $a = b = 2$ and $a = 4, b = 2$, for example, what kind of plain-English prior beliefs about the movie does each beta pdf encode?\n",
    "\n",
    "\n",
    "- **Implementation/Analysis:** How does the choice of prior affect the posterior distribution of the 'likability' for the movies: *Toy Story, Star Wars, The Shawshank Redemption, Down Periscope and Chain Reaction*.\n",
    "\n",
    "   **Hint:** Use our posterior sampling function to visualize the posterior distribution.\n",
    "   \n",
    " \n",
    "- **Implementation/Analysis:** How does the effect of the prior on the posterior distribution vary with the number of user ratings? \n",
    "\n",
    "   **Hint:** Visualize the posterior distribution for different sizes of subsample of user ratings for the movie *Star Wars*.\n",
    "   \n",
    "In the following, we've provide you a couple of functions for visualize beta priors and approximating their associated posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------  plot_beta_prior\n",
    "# A function to visualize a beta pdf on a set of axes\n",
    "# Input: \n",
    "#      a (parameter controlling shape of beta prior)\n",
    "#      b (parameter controlling shape of beta prior)\n",
    "#      color (color of beta pdf)\n",
    "#      ax (axes on which to plot pdf)\n",
    "# Returns: \n",
    "#      ax (axes with plot of beta pdf)\n",
    "\n",
    "def plot_beta_prior(a, b, color, ax):\n",
    "    rv = sp.stats.beta(a, b)\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    ax.plot(x, rv.pdf(x), '-', lw=2, color=color, label='a=' + str(a) + ', b=' + str(b))\n",
    "    ax.set_title('Beta prior with a=' + str(a) + ', b=' + str(b))\n",
    "    ax.legend(loc='best')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------  sample_posterior\n",
    "# A function that samples points from the posterior over a movie's \n",
    "# likability, given a binomial likelihood function and beta prior\n",
    "# Input: \n",
    "#      a (parameter controlling shape of beta prior)\n",
    "#      b (parameter controlling shape of beta prior)\n",
    "#      likes (the number of likes in likelihood)\n",
    "#      ratings (total number of ratings in likelihood)\n",
    "#      n_samples (number of samples to take from posterior)\n",
    "# Returns: \n",
    "#      post_samples (a array of points from the posterior)\n",
    "\n",
    "def sample_posterior(a, b, likes, ratings, n_samples):\n",
    "    post_samples = np.random.beta(a + likes, b + ratings - likes, n_samples)\n",
    "    return post_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1(c): Recommendation based on ranking\n",
    "\n",
    "- **Implementation:** Choose a reasonable beta prior, choose a reasonable statistic to compute from the posterior, and then build a list of top 25 movies that you would recommend to a new user based on your chosen posterior statistic.  \n",
    "\n",
    " \n",
    "- **Analysis:** How does your top 25 movies compare with the list you obtained in part(a)? Which method of ranking is better?\n",
    "\n",
    " \n",
    "- **Analysis:** So far, our estimates of the 'likability' for a movie was based on the ratings provided by all users. What can be the draw back of this method? How can we improve the recommender system for individual users (if you feel up to the challenge, implement your improved system and compare it to the one you built in the above)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Predicting Urban Demographic Changes\n",
    "\n",
    "### Part 2(a): Temporal patterns in urban demographics\n",
    "\n",
    "In this problem you'll work with some neighborhood demographics of a region in Boston from the years 2000 to 2010. \n",
    "\n",
    "The data you need are in the files `dataset_1_year_2000.txt`, ..., `dataset_1_year_2010.txt`. The first two columns of each dataset contain the adjusted latitude and longitude of some randomly sampled houses. The last column contains economic status of a household: \n",
    "\n",
    "0: low-income, \n",
    "\n",
    "1: middle-class, \n",
    "\n",
    "2: high-income \n",
    "\n",
    "Due to the migration of people in and out of the city, the distribution of each economic group over this region changes over the years. The city of Boston estimates that in this region there is approximately a 25% yearly increase in high-income households; and a 25% decrease in the remaining population, with the decrease being roughly the same amongst both the middle class and lower income households.\n",
    "\n",
    "Your task is to build a model for the city of Boston that is capable of predicting the economic status of a household based on its geographical location. Furthermore, your method of prediction must be accurate over time (through 2010 and beyond). \n",
    "\n",
    "**Hint:** look at data only from 2000, and consider using both Linear Discriminant Analysis (LDA) and Logistic Regression. Is there a reason one method would more suited than the other for this task?\n",
    "\n",
    "**Hint:** how well do your two models do over the years? Is it possible to make use of the estimated yearly changes in proportions of the three demographic groups to improve the predictive accuracy of each models over the years? \n",
    "\n",
    "To help you visually interpret and assess the quality of your classifiers, we are providing you a function to visualize a set of data along with the decision boundaries of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  plot_decision_boundary\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      x (predictors)\n",
    "#      y (labels)\n",
    "#      poly_flag (a boolean parameter, fits quadratic model if true, otherwise linear)\n",
    "#      title (title for plot)\n",
    "#      ax (a set of axes to plot on)\n",
    "# Returns: \n",
    "#      ax (axes with data and decision boundaries)\n",
    "\n",
    "def plot_decision_boundary(x, y, model, poly_flag, title, ax):\n",
    "    # Plot data\n",
    "    ax.scatter(x[y == 1, 0], x[y == 1, 1], c='b')\n",
    "    ax.scatter(x[y == 0, 0], x[y == 0, 1], c='r')\n",
    "    \n",
    "    # Create mesh\n",
    "    interval = np.arange(0,1,0.01)\n",
    "    n = size(interval)\n",
    "    x1, x2 = meshgrid(interval, interval)\n",
    "    x1 = x1.reshape(-1, 1)\n",
    "    x2 = x2.reshape(-1, 1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # Predict on mesh points\n",
    "    if(poly_flag):\n",
    "        quad_features = preprocessing.PolynomialFeatures(degree=2)\n",
    "        xx = quad_features.fit_transform(xx)\n",
    "    yy = model.predict(xx)    \n",
    "    yy = yy.reshape((n, n))\n",
    "\n",
    "    # Plot decision surface\n",
    "    x1 = x1.reshape(n, n)\n",
    "    x2 = x2.reshape(n, n)\n",
    "    ax.contourf(x1, x2, yy, alpha=0.1)\n",
    "    \n",
    "    # Label axes, set title\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2(b): Geographic patterns in urban demographics\n",
    "\n",
    "In `dataset_2.txt` and `dataset_3.txt` you have the demographic information for a random sample of houses in two regions in Cambridge. There are only two economic brackets for the households in these datasets: \n",
    "\n",
    "0: low-income or middle-class, \n",
    "\n",
    "1 - high-income. \n",
    "\n",
    "For each region, recommend a classification model, chosen from all the ones you have learned, that is most appropriate for classifying the demographics of households in the region.\n",
    "\n",
    "**Hint:** Support your answers with both numerical and visual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Regularization\n",
    "\n",
    "We have seen ways to include different forms of regularizations in Linear regression and Logistic regression, in order to avoid overfitting. We will now explore ways to incorporate regularization within the discriminant analysis framework.\n",
    "\n",
    "- When we have a small training sample, we end up with poor estimates of the class proportions $\\pi_i$ and covariance matrices $\\Sigma$. How can we regularize these quantities to improve the quality of the fitted model?\n",
    "\n",
    "\n",
    "- We have seen that different assumptions on the covariance matrix results in either a linear or quadratic decision boundary. While the former may yield poor prediction accuracy, the latter could lead to over-fitting. Can you think of a suitable way to regularize the covariance to have an intermediate fit?\n",
    "\n",
    "The solutions that you suggest must include a parameter that allows us to control the amount of regularization.\n",
    "\n",
    "Be detailed in your explanation and support your reasoning fully. You do not, however, need to implement any of these solutions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
